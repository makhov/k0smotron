{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"k0smotron - The Kubernetes control plane manager","text":"<p>From pets to cattle: Streamline your Kubernetes control plane management with k0smotron</p>"},{"location":"#features","title":"Features","text":""},{"location":"#kubernetes-in-kubernetes","title":"Kubernetes-in-Kubernetes","text":"<p>k0smotron allows you to easily create and manage the clusters in an existing Kubernetes cluster. This allows unparalled scalability and flexibility when you have to work with many clusters. It allows truly homogenous setup for all control planes and thus eases the maintenance burden.</p>"},{"location":"#true-control-and-worker-plane-separation","title":"True control and worker plane separation","text":"<p>Using k0smotron the clusters controlplane and workerplane are truly separated. The controlplane, running on an existing cluster has no direct networking connection to the workerplane. This is similar patter nhow all the major cloud providers separate the control and worker planes on the managed clusters. </p>"},{"location":"#bring-your-own-workers","title":"Bring your own workers","text":"<p>With k0smotron you can connect worker nodes from ANY infrastructure to your cluster control plane. </p>"},{"location":"#how-does-it-work","title":"How does it work","text":"<p>You install k0smotron operator into an existing Kubernetes cluster. k0smotron operator will create and manage k0s control planes in that cluster. It leverages the natural pattern of working with custom resources to manage the lifecycle of the k0s control planes. k0smotron will automatically create all the needed Kubernetes lower level constructs, such as pods, configmaps etc., to </p> <p>k0smotron is an Kubernetes operator designed to manage the lifecycle of k0s control planes in a Kubernetes (any distro) cluster. By running the control plane on a k8s cluster we can enjoy and leverage the high availability and auto-healing functionalities of the underlying cluster, a.k.a Mothership.</p> <p></p>"},{"location":"#use-cases","title":"Use cases","text":""},{"location":"#cicd","title":"CI/CD","text":"<p>Often when running integration and end-to-end testing for your software running in Kubernetes you need somewhat temporary clusters in CI. Why not leverage the true flexibility and create those clusters on-demand using k0smotron. Creating a controlplane is as easy as creating a custom resource, so is the deletion of it. No more long living snowflake clusters for CI purposes.</p>"},{"location":"#edge","title":"Edge","text":"<p>Running Kubernetes on the network edge usually means running in low resource infrastructure. What this often means is that setting up the controlplane is either a challenge or a mission impossible. Running the controlplane on a existing cluster, on a separate dedicated infrastructure, removes that challenge and let's you focus on the real edge. </p> <p>Running on the edge often also means large number of clusters to manage. Do you really want to dedicate nodes for each cluster controlplane and manage all the infrastructure for those?</p>"},{"location":"#multi-cloud","title":"Multi-cloud","text":"<p>With k0smotron you can run your control plane management cluster (a.k.a Mothership) in one cloud provider and the workloads in various other cloud providers. This allows you to build and maintain a very streamlined approach to multi cloud.</p>"},{"location":"cluster/","title":"Creating a cluster","text":"<p>The following example creates a simple cluster named <code>k0smotron-test</code>:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: k0smotron.io/v1beta1\nkind: Cluster\nmetadata:\n  name: k0smotron-test\nspec: null\nEOF\n</code></pre> <p>This triggers k0smotron controllers to setup the control plane in pods. Once k0smotron is done you can get the admin access kubeconfig:</p> <pre><code>kubectl get secret kmc-admin-kubeconfig-k0smotron-test -o jsonpath='{.data.value}' | base64 -d &gt; ~/.kube/child.conf\n</code></pre> <p>Once your control plane is ready you can start adding worker nodes into the newly created control plane.</p>"},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#configuration-file-reference","title":"Configuration file reference","text":"<p>The default k0smotron configuration file is a YAML file that contains the following values:</p> <pre><code>apiVersion: k0smotron.io/v1beta1\nkind: Cluster\nmetadata:\nname: k0smotron-test\nspec:\nreplicas: 1\nk0sImage: k0sproject/k0s\nK0sVersion: v1.27.1-k0s.0\nservice:\ntype: NodePort\napiPort: 30443\nkonnectivityPort: 30132\npersistence:\ntype: emptyDir\ncniPlugin: kuberouter\n</code></pre>"},{"location":"configuration/#spec-key-detail","title":"<code>spec</code> Key Detail","text":"Element Description <code>replicas</code> Replicas is the desired number of replicas of the k0s control planes. If unspecified, defaults to 1. If the value is above 1, k0smotron requires kine datasource URL to be set. <code>k0sImage</code> The k0s image to be deployed. <code>K0sVersion</code> The k0s version to be deployed. <code>externalAddress</code> ExternalAddress defines k0s external address. See https://docs.k0sproject.io/stable/configuration/#specapi <code>service</code> <code>Service</code> defines the service configuration. <code>persistence</code> <code>Persistence</code> defines the persistence configuration. <code>kineDataSourceURL</code> Defines the kine datasource URL. Required for HA controlplane setup. Must be set if replicas &gt; 1. <code>cniPlugin</code> Defines the CNI plugin to be used. Possible values are KubeRouter and Calico. Uses KubeRouter by default. Cannot be modified after deploying the cluster."},{"location":"configuration/#specservice","title":"<code>spec.service</code>","text":"Element Description <code>type</code> Service type. Possible values: <code>NodePort</code>,<code>LoadBalancer</code> <code>apiPort</code> Defines the kubernetes API port. <code>konnectivityPort</code> Defines the konnectivity port."},{"location":"configuration/#specpersistence","title":"<code>spec.persistence</code>","text":"Element Description <code>type</code> Persistence type. Possible values: <code>emptyDir</code>,<code>hostPath</code>,<code>pvc</code> <code>hostPath</code> Defines the host path configuration. Will be used as is in case of <code>.spec.persistence.type</code> is <code>hostPath</code>. <code>persistentVolumeClaim</code> Defines the PVC configuration. Will be used as is in case of <code>.spec.persistence.type</code> is <code>pvc</code>."},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#how-is-k0smotron-different-from-typical-multi-cluster-management-solutions-such-as-tanzu-rancher-etc","title":"How is k0smotron different from typical multi-cluster management solutions such as Tanzu, Rancher etc.?","text":"<p>Most of the existing multi-cluster management solutions provision specific infrastructure for the control planes, in most cases VMs. In all of the cases we've looked at the worker plane infrastructure is also provisioned in the same infrastructure with the control plane and thus not allowing you to fully utilize the capabilities of the management cluster.</p>"},{"location":"faq/#how-is-this-different-for-managed-kubernetes-providers","title":"How is this different for managed Kubernetes providers?","text":"<ul> <li>Control and Flexibility: k0smotron gives you full control over your cluster configurations within your existing Kubernetes cluster, offering unparalleled flexibility.</li> <li>Bring Your Own Workers: Unlike managed Kubernetes providers, k0smotron allows you to connect worker nodes from any infrastructure, providing greater freedom and compatibility.</li> <li>Cost Efficiency: By leveraging your existing Kubernetes cluster, k0smotron helps reduce costs associated with managing separate clusters or paying for additional resources.</li> <li>Homogeneous Setup: k0smotron ensures a consistent configuration across clusters, simplifying maintenance and management tasks.</li> </ul>"},{"location":"faq/#what-is-the-relation-of-k0smotron-with-cluster-api","title":"What is the relation of k0smotron with Cluster API?","text":"<p>While k0smotron currently is a \"standalone\" controller for k0s control planes we're looking to expand this as a full Cluster API provider. Or rather set pf providers as were looking to implement both ControlPlane and Bootstrap providers.</p>"},{"location":"ha/","title":"Highly available controlplanes","text":"<p>As the nature of Kubernetes workloads, in this case the cluster control planes, is quite dynamic it poses a challenge to setup highly available Etcd cluster for the control plane. In k0smotron we're solving the challenge by \"externalizing\" the control plane data storage HA setup.</p> <p>The control planes managed by <code>k0smotron</code> are k0s control planes. As k0s comes with support for using SQL DBs as data store (via Kine) you can use HA databases instead of Etcd. This enables you to use e.g. Postgres operator, MySQL operator or cloud provider managed databases as the data store for the control planes.</p>"},{"location":"ha/#using-postgres-operator","title":"Using Postgres operator","text":"<p>In this example we show how to use Postgres operator to manage the control plane data store.</p> <p>Install the operator following the quicstart guide.</p> <p>Create the database with a custom resource: <pre><code>apiVersion: \"acid.zalan.do/v1\"\nkind: postgresql\nmetadata:\n  name: acid-minimal-cluster\nspec:\n  teamId: \"acid\"\n  volume:\n    size: 10Gi\n  numberOfInstances: 2\n  users:\n    # database owner\n    k0smotron:\n    - superuser\n    - createdb\n\n  databases:\n    kine: k0smotron\n  postgresql:\n    version: \"15\"\n</code></pre></p> <p>Once the database has been setup properly, you can instruct k0smotron to create a control plane using it:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: k0smotron.io/v1beta1\nkind: Cluster\nmetadata:\n  name: k0smotron-test\nspec:\n  replicas: 3\n  service:\n    type: LoadBalancer\n  kineDataSourceURL: postgres://k0smotron:&lt;passwd&gt;@acid-minimal-cluster.default:5432/kine?sslmode=disable\nEOF\n</code></pre> <p>Note: We know the DB URL exposes the database password now in a plain Kubernetes resource. We're working on a solution to be able to refer it from a secret.</p>"},{"location":"install/","title":"Installation","text":"<p>To install k0smotron, run the following command:</p> <pre><code>kubectl apply -f https://docs.k0smotron.io/v0.3.1/install.yaml\n</code></pre> <p>This install the k0smotron controller manager, all the related CRD definitions and needed RBAC rules.</p> <p>Once the installation is completed you are ready to create your first control planes.</p>"},{"location":"join-nodes/","title":"Join worker nodes","text":"<p>Joining worker nodes is pretty much the exact same process as with k0s in general. You need a join token that enables mutual trust between the worker and controller(s) and which allows the node to join the cluster as worker.</p>"},{"location":"join-nodes/#join-tokens","title":"Join Tokens","text":"<p>To get a token, create a <code>JoinTokenRequest</code> resource:</p> <pre><code>apiVersion: k0smotron.io/v1beta1\nkind: JoinTokenRequest\nmetadata:\nname: my-token\nnamespace: default\nspec:\nclusterRef:\nname: my-cluster\nnamespace: default\n</code></pre> <p>The <code>JoinTokenRequest</code> resource will be processed by the controller and a <code>Secret</code> will be created:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: my-token\nnamespace: default\nlabels:\nk0smotron.io/cluster: my-cluster.default\nk0smotron.io/role: worker\nk0smotron.io/token-request: my-token\ntype: Opaque\ndata:\ntoken: &lt;base64-encoded-token&gt;\n</code></pre> <p>The <code>token</code> field contains the base64-encoded token that can be used to join a worker node to the cluster.</p> <p>To get the decoded token you can use:</p> <pre><code>kubectl get secret my-token -o jsonpath='{.data.token}' | base64 -d\n</code></pre>"},{"location":"join-nodes/#join-nodes","title":"Join nodes","text":"<p>First you need to get the <code>k0s</code> binary on the node:</p> <pre><code>curl -sSLf https://get.k0s.sh | sudo sh\n</code></pre> <p>The download script accepts the following environment variables:</p> Variable Purpose <code>K0S_VERSION=v{{ no such element: dict object['k8s_version'] }}+k0s.0</code> Select the version of k0s to be installed <code>DEBUG=true</code> Output commands and their arguments at execution. <p>Note: Match the k0s version to the version of the control plane you've created.</p> <p>To join the worker, run k0s in the worker mode with the join token you created:</p> <pre><code>sudo k0s install worker --token-file /path/to/token/file\n</code></pre> <pre><code>sudo k0s start\n</code></pre>"},{"location":"join-nodes/#invalidating-tokens","title":"Invalidating tokens","text":"<p>You can limit the validity period by setting the <code>expiry</code> field in the <code>JoinTokenRequest</code> resource:</p> <pre><code>apiVersion: k0smotron.io/v1beta1\nkind: JoinTokenRequest\nmetadata:\nname: my-token\nnamespace: default\nspec:\nclusterRef:\nname: my-cluster\nnamespace: default\nexpiry: 1h\n</code></pre> <p>To invalidate an issued token, delete the <code>JoinTokenRequest</code> resource:</p> <pre><code>kubectl delete jointokenrequest my-token\n</code></pre>"},{"location":"status/","title":"Project status","text":"<p>We\u2019re really early on with the project and as always with any young project there are probably some sharp corners. But with the help of the open source community we plan to iron out those in the coming months.</p> <p>At this point we can not give much guarantees over backwards and forwards compatibility so expect things to break a bit with upcoming releases of k0smotron.</p> <p>We wanted to release k0smotron as early as possible to see whether there is in general interest of running k0s control planes as Kubernetes resources. We are excited about the potential that k0smotron brings to the table, and we look forward to seeing how it can transform the way you manage your Kubernetes deployments.</p>"},{"location":"status/#cluster-api","title":"Cluster API","text":"<p>One of the directions we're looking at is the have k0smotron working as a Cluster API provider for both <code>ControlPlane</code> and worker <code>Bootstrap</code> providers. What this means is that you could utilize Cluster API to provision both the control plane, within the management cluster, and worker nodes in your favourite infrastructure supporting cluster API.</p>"},{"location":"status/#known-limitations","title":"Known limitations","text":"<p>Some of the areas we know have lot of shortcomings currently: - Control Plane configurability: As you know, k0s itself has lot of configurability, we plan to enable full configurability of the k0s control plane - Control plane exposing: Currently k0smotron only supports <code>NodePort</code> and <code>LoadBalancer</code> type services. While that is in itself quite ok quite often there's need to further configure e.g. annotations etc. on the created service to make them play nice with cloud provider implementations. - Updates: While k0smotron is able to update the cluster controlplane easily the update does not strecth into worker nodes. - </p>"}]}